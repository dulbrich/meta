Sorting Applications

INTRODUCTION

Searching through a sorted array is faster than an unsorted array.
Can find duplicates easily using sorting
We can find the median or top or smallest item if sorted.

Question Notes:
- If an array is not sorted, then we can find the range, min, max and mean (average) of all the numbers in a linear scan. It would be hard without extra space to find the median (middle number). It would also take O(n) to find any given number since it's not sorted.
- If an array is sorted, then we can find the range, min, max and mean as well as the median very easily. With binary search we could find any given number in O(log n)

SELECTION SORT

- scan through the entire array to find the smallest value, then put it in the first place. Then scan the rest of the array and put it in the 2nd place and so on. This is brute force.

Pseudo Code:
function selectionsort(A): 
	for i in 0 to n-1:
		minvalue = A[i]
		minindex = i
		for red in i+1 to n-1:
			if A[red] < minvalue:
				minvalue = A[red]
				minindex = red
		swap A[i], A[minindex]
	return A

Time: O(n^2)
Space: O(n)

NOTES ON CODING SELECTION SORT IN PYTHON:
- Swap is as easy as A[i], A[minindex] = A[minindex], A[i]
- for i in 0 to n-1 translates to for i in range(len(A))
- for red in i+1 to n-1 translates to for red in range(i+1, len(A))
- In general make sure you have all the parts from the pseudo code correct when coding. Translation errors tripped me up and cost time.

BASICS OF ASYMPTOTIC ANALYSIS

- Actual run time of a program will vary based on hardware, language and so on.
- We measure run time an algorithm based on the number of basic operations in pseudocode, in a way that it can be counted even before writing it as a program.
- Runtime of selection sort is actually T(n) = an^2 + bn + c where a, b, and c are some system constants. bn + c are considered lower-order terms and as n becomes large, their contribution to run time actually becomes very close to zero comparitively. Therefore we drop those term. We also drop all the system constants. In this way we end up with only n^2 runtime. We write that as Theta(n^2) or O(n^2).

BRUTE FORCE: BUBBLE SORT

Pseudo Code:
function bubblesort(A):
	for i in 0 to n-1:
		for red in n-1 down to i+1:
			if A[red-1] > A[red]:
				swap A[red-1], A[red]
	return A

Time: O(n^2) <-- worse than selection sort because it has more swapping
Space: O(n)

DECREASE AND CONQUER: INSERTION SORT

Design Strategy #1: Brute Force
- Try to be 'exhaustive'.
Design Strategy #2: Decrease and Conquer
- Decrease the given problem of size n to size n-1 (solve for smallest case and then let iteration solve the rest)

Pseudo Code:
function insertionsort(A):
	for i in 0 to n-1:
		temp = A[i]
		while red >= 0 and A[red] > temp:
			A[red+1] = A[red]
			red --
		A[red+1] = temp
	return A

DIVIDE & CONQUER: MERGE SORT

- instead of reducing the problem to n-1 we divide the problem space in half (n/2)
- this is merge sort because we merge two halves together.

Pseudo Code:
function mergesort(A):
	helper(A, 0, length(A)-1)
	return A
function helper(A, start, end):
	# leaf worker
	if start == end:
		return
	# internal node worker
	mid = (start + end)/2
	helper(A, start, mid)
	helper(A, mid+1, end)
	//merge the two sorted halves
	i = start, j = mid+1
	aux = en empty array of size end-start+1
	while i <= mid and j <= end:
		if A[i] <= A[j]:
			aux.append(A[i])
			i++
		else: // A[i] > A[j]
			aux.append(A[j])
			j++
	// gather phase
	while i <= mid:
		aux.append(A[i])
		i++
	while j <= end:
		aux.append(A[j])
		j++
	A[start...end] <-- aux
	return

- Run time is determined by the height of the tree (log n) multiplied by the size of the array (n)
- Run time: O(n log n)
- Big Omega notation shows the best case (or lower bound)
- Merge sort needs extra space (not in place algorithm) 
